{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI HAR Dataset Analysis with Wearable Pipeline\n",
    "\n",
    "This notebook demonstrates how to use the UCI Human Activity Recognition dataset with our wearable sensor analysis pipeline.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The UCI HAR dataset contains sensor data from smartphones worn by 30 volunteers performing six activities:\n",
    "- WALKING\n",
    "- WALKING_UPSTAIRS\n",
    "- WALKING_DOWNSTAIRS\n",
    "- SITTING\n",
    "- STANDING\n",
    "- LAYING\n",
    "\n",
    "**Key Features:**\n",
    "- 30 subjects (volunteers)\n",
    "- 6 activities\n",
    "- 50Hz sampling rate\n",
    "- 2.56-second windows (128 samples)\n",
    "- Pre-computed features (561 features)\n",
    "- Raw sensor signals available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Import our pipeline components\n",
    "from load_uci_har import UCIHARLoader, load_uci_har_for_pipeline\n",
    "from data_loader import DataLoader\n",
    "from feature_engineer import FeatureEngineer\n",
    "from model import WearableModel\n",
    "from evaluator import ModelEvaluator\n",
    "from visualizer import ResultVisualizer\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load UCI HAR Dataset\n",
    "\n",
    "The pipeline automatically downloads the UCI HAR dataset if it's not found locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize UCI HAR loader\n",
    "uci_loader = UCIHARLoader(data_dir=\"../data/raw/UCI_HAR\")\n",
    "\n",
    "# Get dataset information\n",
    "dataset_info = uci_loader.get_dataset_info()\n",
    "print(\"üìä UCI HAR Dataset Information:\")\n",
    "print(f\"Name: {dataset_info['name']}\")\n",
    "print(f\"Activities: {dataset_info['activities']}\")\n",
    "print(f\"Subjects: {dataset_info['n_subjects']}\")\n",
    "print(f\"Sampling Rate: {dataset_info['sampling_rate']} Hz\")\n",
    "print(f\"Window Size: {dataset_info['window_size']} seconds\")\n",
    "print(f\"\\nDescription: {dataset_info['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset optimized for our pipeline\n",
    "print(\"üîÑ Loading UCI HAR dataset...\")\n",
    "raw_data = load_uci_har_for_pipeline(\n",
    "    data_dir=\"../data/raw/UCI_HAR\",\n",
    "    auto_download=True,\n",
    "    convert_format=True,\n",
    "    include_raw_signals=False  # Set to True if you want raw signals\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"Columns: {raw_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset statistics\n",
    "print(\"üìà Dataset Overview:\")\n",
    "print(f\"Total samples: {len(raw_data):,}\")\n",
    "print(f\"Unique subjects: {raw_data['user_id'].nunique()}\")\n",
    "print(f\"Unique activities: {raw_data['activity'].nunique()}\")\n",
    "\n",
    "print(\"\\nüéØ Activity Distribution:\")\n",
    "activity_counts = raw_data['activity'].value_counts()\n",
    "for activity, count in activity_counts.items():\n",
    "    percentage = (count / len(raw_data)) * 100\n",
    "    print(f\"{activity}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nüë• Subject Distribution:\")\n",
    "subject_counts = raw_data['user_id'].value_counts()\n",
    "print(f\"Min samples per subject: {subject_counts.min():,}\")\n",
    "print(f\"Max samples per subject: {subject_counts.max():,}\")\n",
    "print(f\"Average samples per subject: {subject_counts.mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activity distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Activity distribution bar chart\n",
    "activity_counts.plot(kind='bar', ax=axes[0], color='skyblue', alpha=0.8)\n",
    "axes[0].set_title('Activity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_xlabel('Activity')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "total_samples = len(raw_data)\n",
    "for i, (activity, count) in enumerate(activity_counts.items()):\n",
    "    percentage = (count / total_samples) * 100\n",
    "    axes[0].text(i, count + total_samples*0.01, f'{percentage:.1f}%', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Activity distribution pie chart\n",
    "activity_counts.plot(kind='pie', ax=axes[1], autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('Activity Proportion', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze accelerometer data distribution\n",
    "if all(col in raw_data.columns for col in ['acc_x', 'acc_y', 'acc_z']):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Individual axis distributions\n",
    "    for i, axis in enumerate(['acc_x', 'acc_y', 'acc_z']):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        # Plot distribution for each activity\n",
    "        for activity in raw_data['activity'].unique():\n",
    "            activity_data = raw_data[raw_data['activity'] == activity][axis]\n",
    "            ax.hist(activity_data, bins=30, alpha=0.6, label=activity, density=True)\n",
    "        \n",
    "        ax.set_title(f'{axis.upper()} Distribution by Activity')\n",
    "        ax.set_xlabel(f'{axis} (g)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined 3D acceleration magnitude\n",
    "    ax = axes[1, 1]\n",
    "    raw_data['acc_magnitude'] = np.sqrt(raw_data['acc_x']**2 + raw_data['acc_y']**2 + raw_data['acc_z']**2)\n",
    "    \n",
    "    for activity in raw_data['activity'].unique():\n",
    "        activity_data = raw_data[raw_data['activity'] == activity]['acc_magnitude']\n",
    "        ax.hist(activity_data, bins=30, alpha=0.6, label=activity, density=True)\n",
    "    \n",
    "    ax.set_title('Acceleration Magnitude Distribution by Activity')\n",
    "    ax.set_xlabel('Acceleration Magnitude (g)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Raw accelerometer data not available in converted format\")\n",
    "    print(\"Showing first few feature columns instead:\")\n",
    "    numeric_cols = raw_data.select_dtypes(include=[np.number]).columns[:6]\n",
    "    raw_data[numeric_cols].hist(figsize=(15, 8), bins=30)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline components\n",
    "data_loader = DataLoader()\n",
    "feature_engineer = FeatureEngineer(\n",
    "    window_size=25,  # 1 second at 25Hz (adjusted from UCI HAR's 128 samples)\n",
    "    overlap=0.5,     # 50% overlap\n",
    "    sampling_rate=25.0  # Downsample from 50Hz to 25Hz for consistency\n",
    ")\n",
    "\n",
    "print(\"üîÑ Preprocessing data...\")\n",
    "processed_data = data_loader.preprocess_data(raw_data)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete!\")\n",
    "print(f\"Original shape: {raw_data.shape}\")\n",
    "print(f\"Processed shape: {processed_data.shape}\")\n",
    "print(f\"Columns added: {set(processed_data.columns) - set(raw_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "print(\"üîÑ Extracting features...\")\n",
    "print(\"This may take a few minutes for the full UCI HAR dataset...\")\n",
    "\n",
    "# Extract features grouped by user for better validation\n",
    "features_df, labels = feature_engineer.extract_features(\n",
    "    processed_data, \n",
    "    group_by='user_id'  # Process each user separately\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Feature extraction complete!\")\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Number of feature windows: {len(features_df)}\")\n",
    "print(f\"Number of features per window: {features_df.shape[1]}\")\n",
    "print(f\"Label distribution: {pd.Series(labels).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze extracted features\n",
    "print(\"üîç Feature Analysis:\")\n",
    "\n",
    "# Group features by type\n",
    "feature_groups = feature_engineer.get_feature_groups()\n",
    "for group_name, features in feature_groups.items():\n",
    "    print(f\"{group_name}: {len(features)} features\")\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\nüìä Analyzing feature importance...\")\n",
    "importance_df = feature_engineer.analyze_feature_importance(features_df, labels)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10)[['feature', 'mutual_info', 'variance']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train models\n",
    "print(\"ü§ñ Training machine learning models...\")\n",
    "print(\"This will take several minutes with hyperparameter tuning...\")\n",
    "\n",
    "model = WearableModel(random_state=42)\n",
    "\n",
    "# Train multiple models\n",
    "model_results = model.train(\n",
    "    features_df, \n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    cv_folds=5,\n",
    "    models_to_train=['Random Forest', 'Logistic Regression', 'Gradient Boosting']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete!\")\n",
    "print(f\"Best model: {model.best_model_name}\")\n",
    "\n",
    "# Display results summary\n",
    "print(\"\\nüìà Model Performance Summary:\")\n",
    "for model_name, result in model_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print(f\"  CV Score: {result['cv_mean']:.4f} ¬± {result['cv_std']:.4f}\")\n",
    "    print(f\"  F1-Score: {result['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation\n",
    "print(\"üìä Detailed Model Evaluation:\")\n",
    "\n",
    "evaluator = ModelEvaluator(class_names=list(set(labels)))\n",
    "\n",
    "# Evaluate best model\n",
    "best_result = model_results[model.best_model_name]\n",
    "evaluation = evaluator.evaluate_model(\n",
    "    best_result['y_test'],\n",
    "    best_result['y_test_pred'],\n",
    "    best_result.get('y_test_proba'),\n",
    "    model.best_model_name\n",
    ")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model ({model.best_model_name}) Detailed Results:\")\n",
    "print(f\"Accuracy: {evaluation['accuracy']:.4f}\")\n",
    "print(f\"Precision (weighted): {evaluation['precision_weighted']:.4f}\")\n",
    "print(f\"Recall (weighted): {evaluation['recall_weighted']:.4f}\")\n",
    "print(f\"F1-Score (weighted): {evaluation['f1_weighted']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    best_result['y_test'], \n",
    "    best_result['y_test_pred'],\n",
    "    target_names=model.label_encoder.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer\n",
    "visualizer = ResultVisualizer(results_dir=\"../results/uci_har\")\n",
    "\n",
    "print(\"üé® Creating visualizations...\")\n",
    "\n",
    "# Model comparison\n",
    "visualizer.plot_model_comparison(model_results)\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(best_result['y_test'], best_result['y_test_pred'])\n",
    "visualizer.plot_confusion_matrix(\n",
    "    cm, \n",
    "    model.label_encoder.classes_,\n",
    "    title=f\"UCI HAR - {model.best_model_name}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance visualization\n",
    "if 'feature_importance' in best_result and best_result['feature_importance'] is not None:\n",
    "    visualizer.plot_feature_importance(\n",
    "        best_result['feature_importance'],\n",
    "        features_df.columns.tolist(),\n",
    "        top_n=20,\n",
    "        title=f\"UCI HAR Feature Importance - {model.best_model_name}\"\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Feature importance not available for this model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report visualization\n",
    "if 'classification_report' in best_result:\n",
    "    visualizer.plot_classification_report(best_result['classification_report'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Predictions and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test samples\n",
    "print(\"üîÆ Making predictions on test samples:\")\n",
    "\n",
    "# Get test features and true labels\n",
    "test_indices = np.arange(len(best_result['y_test']))\n",
    "sample_indices = np.random.choice(test_indices, size=10, replace=False)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    true_label = model.label_encoder.inverse_transform([best_result['y_test'][idx]])[0]\n",
    "    pred_label = model.label_encoder.inverse_transform([best_result['y_test_pred'][idx]])[0]\n",
    "    \n",
    "    if best_result.get('y_test_proba') is not None:\n",
    "        confidence = np.max(best_result['y_test_proba'][idx])\n",
    "        status = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "        print(f\"{status} Sample {i+1}: True={true_label}, Pred={pred_label}, Conf={confidence:.3f}\")\n",
    "    else:\n",
    "        status = \"‚úÖ\" if true_label == pred_label else \"‚ùå\"\n",
    "        print(f\"{status} Sample {i+1}: True={true_label}, Pred={pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "if best_result.get('y_test_proba') is not None:\n",
    "    print(\"\\nüìä Prediction Confidence Analysis:\")\n",
    "    \n",
    "    confidences = np.max